name: Publish docker image

on:
  release:
    types: [created]

jobs:
  publish-release:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout latest code
      uses: actions/checkout@v2
    - name: Get the version
      id: get_version
      run: echo ::set-output name=VERSION::${GITHUB_REF/refs\/tags\//}
      # access it through ${{ steps.get_version.outputs.VERSION }}
    - name: Set up JDK 8
      uses: actions/setup-java@v1
      with:
        java-version: 8
    - uses: r-lib/actions/setup-r@v1
      with:
        r-version: '3.6.2'
    - name: install R packages
      run: |
        sudo apt-get install -y texlive-latex-base texlive texlive-fonts-extra texinfo qpdf
        sudo Rscript -e "install.packages(c('curl', 'xml2', 'httr', 'devtools', 'testthat', 'knitr', 'rmarkdown', 'roxygen2', 'e1071', 'survival'), repos='https://cloud.r-project.org/')"
      env:
        DEBIAN_FRONTEND: noninteractive
        DEBCONF_NONINTERACTIVE_SEEN: true
    - uses: actions/setup-python@v1
      with:
        python-version: '3.x'
        architecture: 'x64'
    - name: Apply Patch
      run: |-
        # Remove -s option of tini. while gvisor does not support PR_SET_CHILD_SUBREAPER
        sed -i 's/tini -s/tini/' $SPARK_ENTRYPOINT

        # Add passwd entry. otherwise, entrypoint.sh will shows 'Container ENTRYPOINT failed to add passwd entry for anonymous UID'
        # and executor will fail with  javax.security.auth.login.LoginException: java.lang.NullPointerException: invalid null input: name at com.sun.security.auth.UnixPrincipal.<init>(UnixPrincipal.java:71)
        sed -i '/^USER/d' $BASE_DOCKERFILE
        # install additional jars
        echo 'ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar $SPARK_HOME/jars' >> $BASE_DOCKERFILE
        echo 'ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.563/aws-java-sdk-bundle-1.11.563.jar $SPARK_HOME/jars' >> $BASE_DOCKERFILE
        echo 'RUN chmod 644 $SPARK_HOME/jars/hadoop-aws-3.2.0.jar $SPARK_HOME/jars/aws-java-sdk-bundle-1.11.563.jar' >> $BASE_DOCKERFILE

        # create user and group
        echo 'RUN groupadd --gid $spark_uid spark && useradd -ms /bin/bash spark --uid $spark_uid --gid $spark_uid && chown -R spark:spark /opt/spark/work-dir' >> $BASE_DOCKERFILE
        echo 'USER ${spark_uid}' >> $BASE_DOCKERFILE

        # Patch python Dockerfile to install multiple verison
        cat $PYTHON_DOCKERFILE | sed -n '1,/RUN apt-get update/p;/rm -r \/root\/.cache/,$p' | sed 's/rm -r \/root\/.cache \&\&//g' > /tmp/Dockerfile
        sed -i 's/RUN apt-get update/RUN apt-get update \&\& apt-get install -y curl git zlib1g-dev libssl-dev libreadline-dev gcc make libffi-dev \&\& ln -s \/home\/spark\/.pyenv\/versions\/3.6.9\/bin\/python3 \/usr\/bin\/python \&\& ln -s \/home\/spark\/.pyenv\/versions\/3.6.9\/bin\/python3 \/usr\/bin\/python3/g' /tmp/Dockerfile
        
        cat <<EOT >> /tmp/Dockerfile
        RUN cd /home/spark/ && curl https://pyenv.run | bash && \
            /home/spark/.pyenv/bin/pyenv install 3.6.9 && \
            /home/spark/.pyenv/bin/pyenv install 3.7.7 && \
            /home/spark/.pyenv/bin/pyenv install 3.8.1 && \
            /home/spark/.pyenv/bin/pyenv global 3.6.9 && \
            rm -rf /tmp/python-build*
        EOT

        mv /tmp/Dockerfile $PYTHON_DOCKERFILE

        # print
        cat $SPARK_ENTRYPOINT
        cat $BASE_DOCKERFILE
        cat $PYTHON_DOCKERFILE
      env:
        SPARK_ENTRYPOINT: resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh
        BASE_DOCKERFILE: resource-managers/kubernetes/docker/src/main/dockerfiles/spark/Dockerfile
        PYTHON_DOCKERFILE: resource-managers/kubernetes/docker/src/main/dockerfiles/spark/bindings/python/Dockerfile
    - name: Build distribution
      run: |-
        ./dev/make-distribution.sh --name spark --pip --r --tgz -Psparkr -Phadoop-3.2 -Phive -Phive-thriftserver -Pkubernetes
      env:
        DEBIAN_FRONTEND: noninteractive
        DEBCONF_NONINTERACTIVE_SEEN: true
    - name: Build docker image
      run: |-
        ./bin/docker-image-tool.sh \
        -r opendatastudio \
        -t ${{ steps.get_version.outputs.VERSION }} \
        -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile \
        -R kubernetes/dockerfiles/spark/bindings/R/Dockerfile \
        build
      working-directory: ./dist
    - name: Push image
      run: |-
        docker images
        echo "${{ secrets.DOCKER_PASSWORD }}" | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
        docker push opendatastudio/spark:${{ steps.get_version.outputs.VERSION }}
        docker push opendatastudio/spark-py:${{ steps.get_version.outputs.VERSION }}
        docker push opendatastudio/spark-r:${{ steps.get_version.outputs.VERSION }}
